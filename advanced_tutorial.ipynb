 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG System Tutorial\n",
    "\n",
    "This notebook provides practical examples and explanations for advanced features and customizations of our RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experimenting with Different Document Types\n",
    "\n",
    "Let's explore how the system handles different types of documents and their unique characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_processor import DocumentProcessor\n",
    "import os\n",
    "\n",
    "# Initialize the processor with custom chunk sizes\n",
    "processor = DocumentProcessor(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Example: Process a technical document\n",
    "tech_doc = \"\"\"\n",
    "Technical Specification Document\n",
    "Version: 1.0\n",
    "\n",
    "System Requirements:\n",
    "- CPU: 2.0 GHz or higher\n",
    "- RAM: 8GB minimum\n",
    "- Storage: 20GB free space\n",
    "\n",
    "Installation Steps:\n",
    "1. Download the installer\n",
    "2. Run setup.exe\n",
    "3. Follow the wizard\n",
    "4. Restart your computer\n",
    "\n",
    "Configuration:\n",
    "- Set memory limit to 4GB\n",
    "- Enable auto-updates\n",
    "- Configure backup schedule\n",
    "\"\"\"\n",
    "\n",
    "# Save to a temporary file\n",
    "with open(\"tech_spec.txt\", \"w\") as f:\n",
    "    f.write(tech_doc)\n",
    "\n",
    "# Process the document\n",
    "documents = processor.process_file(\"tech_spec.txt\")\n",
    "print(f\"Number of chunks: {len(documents)}\")\n",
    "print(\"\\nFirst chunk:\")\n",
    "print(documents[0].page_content)\n",
    "\n",
    "# Clean up\n",
    "os.remove(\"tech_spec.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experimenting with Chunk Sizes\n",
    "\n",
    "Different types of content may require different chunk sizes. Let's experiment with various configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chunk_sizes(text, chunk_sizes):\n",
    "    results = {}\n",
    "    for size in chunk_sizes:\n",
    "        processor = DocumentProcessor(chunk_size=size, chunk_overlap=size//5)\n",
    "        documents = processor.process_documents([text])\n",
    "        results[size] = {\n",
    "            \"num_chunks\": len(documents),\n",
    "            \"avg_chunk_size\": sum(len(d.page_content) for d in documents) / len(documents)\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Test text\n",
    "test_text = \"\"\"\n",
    "This is a test document to demonstrate how different chunk sizes affect the processing.\n",
    "We'll see how the system breaks down the text into smaller pieces.\n",
    "Smaller chunks might capture more specific information but lose context.\n",
    "Larger chunks maintain more context but might include irrelevant information.\n",
    "Finding the right balance is crucial for effective retrieval.\n",
    "\"\"\"\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes = [100, 200, 300, 400, 500]\n",
    "results = test_chunk_sizes(test_text, chunk_sizes)\n",
    "\n",
    "print(\"Chunk Size Analysis:\")\n",
    "for size, data in results.items():\n",
    "    print(f\"\\nChunk Size: {size}\")\n",
    "    print(f\"Number of chunks: {data['num_chunks']}\")\n",
    "    print(f\"Average chunk size: {data['avg_chunk_size']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customizing the Prompt Template\n",
    "\n",
    "Let's explore how different prompt templates affect the quality of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_system import RAGSystem\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Different prompt templates\n",
    "templates = {\n",
    "    \"basic\": \"\"\"Use the following context to answer the question:\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    \"detailed\": \"\"\"You are a helpful assistant. Use the following context to provide a detailed answer to the question.\n",
    "If you don't know the answer, say so. Don't make up information.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a comprehensive answer:\"\"\",\n",
    "    \n",
    "    \"technical\": \"\"\"As a technical expert, analyze the following context and provide a precise answer to the question.\n",
    "Focus on technical accuracy and clarity.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Technical Answer:\"\"\"\n",
    "}\n",
    "\n",
    "# Test different prompts\n",
    "def test_prompts(question, context):\n",
    "    results = {}\n",
    "    for name, template in templates.items():\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "        rag = RAGSystem()\n",
    "        rag.qa_chain.chain_type_kwargs[\"prompt\"] = prompt\n",
    "        response = rag.query(question)\n",
    "        results[name] = response[\"answer\"]\n",
    "    return results\n",
    "\n",
    "# Example test\n",
    "test_context = \"The system requires Python 3.8 or higher, 8GB RAM, and 20GB storage.\"\n",
    "test_question = \"What are the system requirements?\"\n",
    "\n",
    "results = test_prompts(test_question, test_context)\n",
    "for name, answer in results.items():\n",
    "    print(f\"\\n{name.title()} Prompt:\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding UI Features\n",
    "\n",
    "Let's explore how to add new features to the Streamlit interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Example of new UI features\n",
    "def enhanced_ui():\n",
    "    st.title(\"Enhanced RAG System Interface\")\n",
    "    \n",
    "    # Document analysis section\n",
    "    st.header(\"Document Analysis\")\n",
    "    uploaded_file = st.file_uploader(\"Upload a document\", type=[\"txt\", \"pdf\"])\n",
    "    if uploaded_file:\n",
    "        # Show document statistics\n",
    "        st.subheader(\"Document Statistics\")\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.metric(\"File Size\", f\"{len(uploaded_file.getvalue()) / 1024:.2f} KB\")\n",
    "        with col2:\n",
    "            st.metric(\"File Type\", uploaded_file.type)\n",
    "    \n",
    "    # Advanced search options\n",
    "    st.header(\"Advanced Search\")\n",
    "    search_type = st.radio(\n",
    "        \"Search Type\",\n",
    "        [\"Semantic Search\", \"Keyword Search\", \"Hybrid Search\"]\n",
    "    )\n",
    "    \n",
    "    # Results visualization\n",
    "    st.header(\"Results Visualization\")\n",
    "    visualization_type = st.selectbox(\n",
    "        \"Visualization Type\",\n",
    "        [\"Text\", \"Graph\", \"Timeline\"]\n",
    "    )\n",
    "    \n",
    "    # Example of how to use these features\n",
    "    st.code(\"\"\"\n",
    "# In your app.py, you can add these features like this:\n",
    "if st.sidebar.checkbox('Show Advanced Options'):\n",
    "    enhanced_ui()\n",
    "    \"\"\", language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploring Different Language Models\n",
    "\n",
    "Let's see how different language models affect the quality of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "def compare_models(question, context):\n",
    "    models = {\n",
    "        \"llama2\": \"llama2:1.3b\",\n",
    "        \"mistral\": \"mistral\",\n",
    "        \"codellama\": \"codellama\"\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            llm = Ollama(model=model)\n",
    "            prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "            response = llm(prompt)\n",
    "            results[name] = response\n",
    "        except Exception as e:\n",
    "            results[name] = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example test\n",
    "test_context = \"The system requires Python 3.8 or higher, 8GB RAM, and 20GB storage.\"\n",
    "test_question = \"What are the system requirements?\"\n",
    "\n",
    "results = compare_models(test_question, test_context)\n",
    "for model, answer in results.items():\n",
    "    print(f\"\\n{model.title()} Model:\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Tips\n",
    "\n",
    "Here are some additional tips for working with the RAG system:\n",
    "\n",
    "1. **Document Preprocessing**\n",
    "   - Clean and normalize text before processing\n",
    "   - Remove unnecessary formatting\n",
    "   - Handle special characters appropriately\n",
    "\n",
    "2. **Chunking Strategies**\n",
    "   - Consider document structure when chunking\n",
    "   - Use overlapping chunks to maintain context\n",
    "   - Adjust chunk size based on content type\n",
    "\n",
    "3. **Prompt Engineering**\n",
    "   - Be specific in your prompts\n",
    "   - Include examples when possible\n",
    "   - Test different prompt variations\n",
    "\n",
    "4. **Performance Optimization**\n",
    "   - Cache embeddings when possible\n",
    "   - Use batch processing for large documents\n",
    "   - Monitor memory usage\n",
    "\n",
    "5. **Error Handling**\n",
    "   - Implement proper error handling\n",
    "   - Log errors for debugging\n",
    "   - Provide user-friendly error messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Features to Explore\n",
    "\n",
    "Here are some advanced features you might want to implement:\n",
    "\n",
    "1. **Document Versioning**\n",
    "   - Track changes in documents\n",
    "   - Maintain version history\n",
    "   - Compare different versions\n",
    "\n",
    "2. **Multi-Modal Support**\n",
    "   - Handle images and text together\n",
    "   - Extract text from images\n",
    "   - Generate image descriptions\n",
    "\n",
    "3. **Custom Embeddings**\n",
    "   - Train custom embedding models\n",
    "   - Fine-tune for specific domains\n",
    "   - Optimize for particular use cases\n",
    "\n",
    "4. **Advanced Retrieval**\n",
    "   - Implement hybrid search\n",
    "   - Add filtering capabilities\n",
    "   - Support complex queries\n",
    "\n",
    "5. **Analytics and Monitoring**\n",
    "   - Track system performance\n",
    "   - Monitor user interactions\n",
    "   - Generate usage reports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}